{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad030df3-d153-4416-96b9-ab17568d4c06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.1\n2.16.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print versions\n",
    "print(mlflow.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922da7cf-1dd6-4f38-95f3-ac6075733917",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, input_dim, discriminator_dim, num_classes, pac = 10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.pac = pac\n",
    "        self.pacdim = input_dim * pac\n",
    "        self.seq = tf.keras.Sequential()\n",
    "        \n",
    "        for item in discriminator_dim:\n",
    "            self.seq.add(layers.Dense(item))\n",
    "            self.seq.add(layers.LeakyReLU(0.2))\n",
    "            self.seq.add(layers.Dropout(0.5))\n",
    "            \n",
    "        self.seq.add(layers.Dense(1)) # Output dimension is 1 for binary classification\n",
    "\n",
    "\n",
    "    def call(self, input_data, classes, training=True):\n",
    "        # Combine data and labels\n",
    "        combined_input = tf.concat([input_data, classes], axis=-1)\n",
    "        tf.debugging.assert_equal(tf.shape(combined_input)[0] % self.pac, 0, message=\"The input batch size is not divisible by pac.\")\n",
    "        combined_input = tf.reshape(combined_input, (-1, self.pacdim))\n",
    "        return self.seq(combined_input, training=training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52ea2e8-4e19-4611-83b4-05b24f8d7125",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generator \n",
    "class Residual(tf.keras.Model):\n",
    "    def __init__(self, i, o):\n",
    "        super(Residual, self).__init__()\n",
    "        self.fc = layers.Dense(o)\n",
    "        self.bn = layers.BatchNormalization()\n",
    "        self.relu = layers.ReLU()\n",
    "\n",
    "    def call(self, input_, training=True):\n",
    "        x = self.fc(input_)\n",
    "        x = self.bn(x, training=training)\n",
    "        x = self.relu(x)\n",
    "        return tf.concat([x, input_], axis=-1)\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, generator_dim, num_features, num_classes):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_features = num_features\n",
    "        self.label_embedding = layers.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "        # Define the generator's layers\n",
    "        self.seq = tf.keras.Sequential()\n",
    "        # Adjust the input dimension to account for the concatenated noise and label embeddings\n",
    "        total_input_dim = embedding_dim + num_classes  # Adjust this based on how you're processing labels\n",
    "\n",
    "        # Define the generator's architecture\n",
    "        for dim in generator_dim:\n",
    "            self.seq.add(layers.Dense(dim, activation='relu'))\n",
    "        # The final layer should match the data_dim\n",
    "        self.seq.add(layers.Dense(num_features))\n",
    "\n",
    "    def call(self, noise, classes, training=True):\n",
    "        label_embedding = self.label_embedding(classes)\n",
    "\n",
    "        # Flatten or reshape the label_embedding if it's not already [batch_size, embedding_dim * num_classes]\n",
    "        label_embedding = tf.reshape(label_embedding, [tf.shape(label_embedding)[0], -1])\n",
    "        combined_input = tf.concat([noise, label_embedding], axis=-1)\n",
    "        return self.seq(combined_input, training=training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92e3a103-ec1b-466f-9979-89f4965ace2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loss Functions and Gradient Penality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d38a89e-cf99-4874-91ab-88f82d6e5e88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loss Functions and Gradient Penality\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "def gradient_penalty(discriminator, batch_size, real_data, fake_data):\n",
    "    alpha = tf.random.uniform([batch_size, 1, 1], 0., 1.)\n",
    "    interpolated = alpha * real_data + (1 - alpha) * fake_data\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        pred = discriminator(interpolated, training=True)\n",
    "    gradients = tape.gradient(pred, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2]))\n",
    "    gp = tf.reduce_mean((norm - 1.) ** 2)\n",
    "    return gp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0071ba61-0aee-443c-a9df-6e4a77648f8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Training Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e45354-d7b9-4518-bdb6-46bc8e20d351",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_data, classes, generator, discriminator, generator_optimizer, discriminator_optimizer, batch_size, noise_dim):\n",
    "    current_batch_size = tf.shape(classes)[0]  # Dynamically determine the batch size\n",
    "    # Convert class labels to one-hot\n",
    "    classes = tf.one_hot(classes, depth=num_classes)\n",
    "\n",
    "    noise = tf.random.normal([current_batch_size, noise_dim])\n",
    "\n",
    "    # Train Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generate fake data\n",
    "        fake_data = generator(noise, classes, training=True)\n",
    "\n",
    "        # Get discriminator outputs for real and fake data\n",
    "        real_output = discriminator(real_data, classes, training=True)\n",
    "        fake_output = discriminator(fake_data, classes, training=True)\n",
    "\n",
    "        # Calculate losses\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        # Ensure the losses are scalar\n",
    "        if not gen_loss.shape == () or not disc_loss.shape == ():\n",
    "            raise ValueError(\"Losses must be scalar values.\")\n",
    "\n",
    "    # Get gradients and apply them\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Check for any None gradients\n",
    "    if None in gradients_of_generator or None in gradients_of_discriminator:\n",
    "        raise ValueError(\"Found None in gradients. The computational graph might be broken.\")\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09053c6f-dca3-42de-86fc-26f2773e44f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe390d7-2d4b-4145-9076-df9b2ce0b0b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_preprocess_dataset(path):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(path, index_col=0).reset_index()\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Separate features and labels\n",
    "    X_scaled = df.drop('Attack_type', axis=1)\n",
    "    y = df['Attack_type']\n",
    "\n",
    "    # Encode labels as integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    return X_scaled, y_encoded, label_encoder.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb730a6-2ce4-410a-891e-88657e6741bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset \n",
    "processed_file_path = '/dbfs/FileStore/m332479/GANs_forCyberSecurity/processed_RT_IOT2022.csv'\n",
    "X, y, classes = load_preprocess_dataset(processed_file_path)\n",
    "num_classes, num_features = len(classes), X.shape[1]\n",
    "num_classes, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "423957e4-017c-4555-b1c7-f84898c514a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ce20de-b7c2-4636-a458-6e25e7a8b404",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e6b0896-ada7-40b2-93a0-66b8fde212cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming 'features' and 'labels' are your DataFrame columns for input features and target labels\n",
    "batch_size = 500\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=len(X_train))\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)  # Drop the last smaller batch\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d05ebc-618c-49b7-9137-273d26cdf103",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "embedding_dim = 128\n",
    "noise_dim = 128\n",
    "discriminator_dim = (256, 256)\n",
    "generator_dim = (256, 256)\n",
    "pac = 10\n",
    "generator_lr = 2e-4\n",
    "discriminator_lr = 2e-4\n",
    "# Within the CTGAN fit method\n",
    "generator = Generator(embedding_dim=noise_dim, generator_dim=generator_dim, num_features=num_features, num_classes=num_classes)\n",
    "discriminator = Discriminator(input_dim=num_features + num_classes, discriminator_dim=discriminator_dim, num_classes=num_classes, pac=pac)\n",
    "\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=generator_lr)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=discriminator_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e35901ec-3ea1-4b8c-b074-a66fc9519a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d53b5b6-c93a-437c-b43b-bdf899eb8b93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2991218399250920'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "mlflow.end_run()\n",
    "# Initializes an MLflow experiment for tracking machine learning tasks. \n",
    "# It checks if the specified experiment already exists and creates it if not, storing the experiment ID.\n",
    "dbutils.widgets.text(\"mlflow_exp_root_path\",\"/Users/m332479@azg.pwus.us/ml_experments\")\n",
    "\n",
    "from mlflow.tracking import MlflowClient\n",
    "mlflow_exp_root_path = dbutils.widgets.get(\"mlflow_exp_root_path\")\n",
    "client = MlflowClient()\n",
    "\n",
    "## Test if experiment already exists\n",
    "exp_name = f\"{mlflow_exp_root_path}/CGAN_Training\"\n",
    "if exp_name in [x.name for x in client.search_experiments()]:\n",
    "    exp = mlflow.set_experiment(exp_name)\n",
    "    experiment_id = exp.experiment_id\n",
    "else:\n",
    "    ## Create an experiment for runs started from a repo notebook\n",
    "    experiment_id = client.create_experiment(f\"{mlflow_exp_root_path}/CGAN_Training\")\n",
    "experiment_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38233139-ac92-4908-a9e9-88c41fac90e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = \"/Users/m332479@azg.pwus.us/ml_experments/CGAN_Training\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
    "    mlflow.log_param(\"noise_dim\", noise_dim)\n",
    "    mlflow.log_param(\"discriminator_dim\", discriminator_dim)\n",
    "    mlflow.log_param(\"generator_dim\", generator_dim)\n",
    "    mlflow.log_param(\"pac\", pac)\n",
    "    mlflow.log_param(\"generator_lr\", generator_lr)\n",
    "    mlflow.log_param(\"discriminator_lr\", discriminator_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81c285ef-ef2d-4fba-83cd-a92963a16c38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Integrating the Training Step - Batch Level Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d4adc8-c0b5-4d07-ad92-3d0d57e7112f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Generator Loss: -0.33261820673942566, Discriminator Loss: -0.3068772256374359\nEpoch: 0, Generator Loss: -0.33911049365997314, Discriminator Loss: -0.47549593448638916\nEpoch: 0, Generator Loss: -0.43156081438064575, Discriminator Loss: -0.5598580241203308\nEpoch: 0, Generator Loss: -0.33539441227912903, Discriminator Loss: -0.617263674736023\nEpoch: 0, Generator Loss: -0.39731818437576294, Discriminator Loss: -0.6865454316139221\nEpoch: 0, Generator Loss: -0.3759970963001251, Discriminator Loss: -1.0297209024429321\nEpoch: 0, Generator Loss: -0.45771703124046326, Discriminator Loss: -1.089312195777893\nEpoch: 0, Generator Loss: -0.46076950430870056, Discriminator Loss: -1.219399094581604\nEpoch: 0, Generator Loss: -0.3842698335647583, Discriminator Loss: -1.4215925931930542\nEpoch: 0, Generator Loss: -0.5545740723609924, Discriminator Loss: -1.2213218212127686\nEpoch: 0, Generator Loss: -0.5517397522926331, Discriminator Loss: -1.4084057807922363\nEpoch: 0, Generator Loss: -0.6448721885681152, Discriminator Loss: -1.487741231918335\nEpoch: 0, Generator Loss: -0.7536600232124329, Discriminator Loss: -1.4741840362548828\nEpoch: 0, Generator Loss: -0.9886474609375, Discriminator Loss: -1.5564556121826172\nEpoch: 0, Generator Loss: -1.001760721206665, Discriminator Loss: -1.6369550228118896\nEpoch: 0, Generator Loss: -1.1162042617797852, Discriminator Loss: -1.6081573963165283\nEpoch: 0, Generator Loss: -1.4339971542358398, Discriminator Loss: -1.5202481746673584\nEpoch: 0, Generator Loss: -1.6194329261779785, Discriminator Loss: -1.4476566314697266\nEpoch: 0, Generator Loss: -1.9164639711380005, Discriminator Loss: -1.182259202003479\nEpoch: 0, Generator Loss: -2.3278446197509766, Discriminator Loss: -1.0526177883148193\nEpoch: 0, Generator Loss: -2.4691929817199707, Discriminator Loss: -0.8663806915283203\nEpoch: 0, Generator Loss: -2.7844085693359375, Discriminator Loss: -0.4858386516571045\nEpoch: 0, Generator Loss: -3.1884896755218506, Discriminator Loss: -0.5455727577209473\nEpoch: 0, Generator Loss: -3.87174916267395, Discriminator Loss: 0.22211527824401855\nEpoch: 0, Generator Loss: -3.786773681640625, Discriminator Loss: -0.09589290618896484\nEpoch: 0, Generator Loss: -4.825530052185059, Discriminator Loss: 1.1782264709472656\nEpoch: 0, Generator Loss: -5.198659896850586, Discriminator Loss: 1.359379768371582\nEpoch: 0, Generator Loss: -5.9275803565979, Discriminator Loss: 2.239649772644043\nEpoch: 0, Generator Loss: -6.367313385009766, Discriminator Loss: 2.5408527851104736\nEpoch: 0, Generator Loss: -7.101657867431641, Discriminator Loss: 3.3314316272735596\nEpoch: 0, Generator Loss: -6.953390598297119, Discriminator Loss: 3.1971309185028076\nEpoch: 0, Generator Loss: -7.816815376281738, Discriminator Loss: 4.259679794311523\nEpoch: 0, Generator Loss: -7.5936279296875, Discriminator Loss: 4.31728458404541\nEpoch: 0, Generator Loss: -8.202607154846191, Discriminator Loss: 4.879205703735352\nEpoch: 0, Generator Loss: -9.118273735046387, Discriminator Loss: 5.998891830444336\nEpoch: 0, Generator Loss: -9.135449409484863, Discriminator Loss: 6.1668291091918945\nEpoch: 0, Generator Loss: -10.246298789978027, Discriminator Loss: 7.452381134033203\nEpoch: 0, Generator Loss: -9.542825698852539, Discriminator Loss: 7.146011829376221\nEpoch: 0, Generator Loss: -7.673551559448242, Discriminator Loss: 5.499755859375\nEpoch: 0, Generator Loss: -8.442970275878906, Discriminator Loss: 6.303661346435547\nEpoch: 0, Generator Loss: -7.34023904800415, Discriminator Loss: 5.738716125488281\nEpoch: 0, Generator Loss: -7.729792594909668, Discriminator Loss: 6.264487266540527\nEpoch: 0, Generator Loss: -6.107729434967041, Discriminator Loss: 4.890440464019775\nEpoch: 0, Generator Loss: -4.182101249694824, Discriminator Loss: 3.458524703979492\nEpoch: 0, Generator Loss: -2.286862850189209, Discriminator Loss: 1.6508790254592896\nEpoch: 0, Generator Loss: -2.4111084938049316, Discriminator Loss: 2.153371572494507\nEpoch: 0, Generator Loss: -1.524961233139038, Discriminator Loss: 1.5827670097351074\nEpoch: 0, Generator Loss: -1.1383297443389893, Discriminator Loss: 1.497192144393921\nEpoch: 0, Generator Loss: 1.945311427116394, Discriminator Loss: -1.308647632598877\nEpoch: 0, Generator Loss: 3.0589044094085693, Discriminator Loss: -1.9465794563293457\nEpoch: 0, Generator Loss: 4.1168718338012695, Discriminator Loss: -2.8247179985046387\nEpoch: 0, Generator Loss: 9.119388580322266, Discriminator Loss: -7.49476957321167\nEpoch: 0, Generator Loss: 10.00550365447998, Discriminator Loss: -7.984987258911133\nEpoch: 0, Generator Loss: 13.839812278747559, Discriminator Loss: -11.475974082946777\nEpoch: 0, Generator Loss: 14.80845832824707, Discriminator Loss: -12.204419136047363\nEpoch: 0, Generator Loss: 18.603046417236328, Discriminator Loss: -15.484375953674316\nEpoch: 0, Generator Loss: 20.398540496826172, Discriminator Loss: -17.0135498046875\nEpoch: 0, Generator Loss: 24.35891342163086, Discriminator Loss: -20.737934112548828\nEpoch: 0, Generator Loss: 27.491554260253906, Discriminator Loss: -24.009052276611328\nEpoch: 0, Generator Loss: 29.716970443725586, Discriminator Loss: -25.666261672973633\nEpoch: 0, Generator Loss: 33.29328918457031, Discriminator Loss: -28.879745483398438\nEpoch: 0, Generator Loss: 34.669368743896484, Discriminator Loss: -29.671157836914062\nEpoch: 0, Generator Loss: 35.26436996459961, Discriminator Loss: -30.12441062927246\nEpoch: 0, Generator Loss: 36.770362854003906, Discriminator Loss: -31.536029815673828\nEpoch: 0, Generator Loss: 38.686256408691406, Discriminator Loss: -32.92631149291992\nEpoch: 0, Generator Loss: 40.70057678222656, Discriminator Loss: -35.024070739746094\nEpoch: 0, Generator Loss: 40.770851135253906, Discriminator Loss: -34.49170684814453\nEpoch: 0, Generator Loss: 39.28812789916992, Discriminator Loss: -33.18628692626953\nEpoch: 0, Generator Loss: 35.92621994018555, Discriminator Loss: -29.487773895263672\nEpoch: 0, Generator Loss: 34.17763900756836, Discriminator Loss: -27.579914093017578\nEpoch: 0, Generator Loss: 31.500612258911133, Discriminator Loss: -24.487567901611328\nEpoch: 0, Generator Loss: 28.07738494873047, Discriminator Loss: -20.680219650268555\nEpoch: 0, Generator Loss: 24.201313018798828, Discriminator Loss: -16.710866928100586\nEpoch: 0, Generator Loss: 20.161258697509766, Discriminator Loss: -12.601479530334473\nEpoch: 0, Generator Loss: 15.669095039367676, Discriminator Loss: -7.971421241760254\nEpoch: 0, Generator Loss: 12.963683128356934, Discriminator Loss: -4.976707458496094\nEpoch: 0, Generator Loss: 11.218822479248047, Discriminator Loss: -3.201780319213867\nEpoch: 0, Generator Loss: 9.008523941040039, Discriminator Loss: -0.4845094680786133\nEpoch: 0, Generator Loss: 7.77557373046875, Discriminator Loss: 0.9604701995849609\nEpoch: 0, Generator Loss: 6.096776962280273, Discriminator Loss: 2.561288833618164\nEpoch: 0, Generator Loss: 3.915469169616699, Discriminator Loss: 5.121386528015137\nEpoch: 0, Generator Loss: 1.8312281370162964, Discriminator Loss: 7.1484174728393555\nEpoch: 0, Generator Loss: -0.6958737373352051, Discriminator Loss: 9.423982620239258\nEpoch: 0, Generator Loss: -3.2714266777038574, Discriminator Loss: 12.563581466674805\nEpoch: 0, Generator Loss: -6.397960662841797, Discriminator Loss: 15.252165794372559\nEpoch: 0, Generator Loss: -10.051823616027832, Discriminator Loss: 18.935863494873047\nEpoch: 0, Generator Loss: -13.744309425354004, Discriminator Loss: 22.538318634033203\nEpoch: 0, Generator Loss: -17.473695755004883, Discriminator Loss: 26.493419647216797\nEpoch: 0, Generator Loss: -21.251117706298828, Discriminator Loss: 29.536865234375\nEpoch: 0, Generator Loss: -27.357572555541992, Discriminator Loss: 35.86140441894531\nEpoch: 0, Generator Loss: -31.836915969848633, Discriminator Loss: 40.529815673828125\nEpoch: 0, Generator Loss: -37.05897521972656, Discriminator Loss: 45.41133499145508\nEpoch: 0, Generator Loss: -42.5739631652832, Discriminator Loss: 50.7052001953125\nEpoch: 0, Generator Loss: -48.11378479003906, Discriminator Loss: 56.30851364135742\nEpoch: 0, Generator Loss: -53.014198303222656, Discriminator Loss: 60.89630126953125\nEpoch: 0, Generator Loss: -58.18954086303711, Discriminator Loss: 66.04508209228516\nEpoch: 0, Generator Loss: -60.11543273925781, Discriminator Loss: 67.69469451904297\nEpoch: 0, Generator Loss: -66.91010284423828, Discriminator Loss: 74.16864776611328\nEpoch: 0, Generator Loss: -72.24884033203125, Discriminator Loss: 79.22051239013672\nEpoch: 0, Generator Loss: -73.95633697509766, Discriminator Loss: 81.1332015991211\nEpoch: 0, Generator Loss: -75.37141418457031, Discriminator Loss: 81.86549377441406\nEpoch: 0, Generator Loss: -73.81995391845703, Discriminator Loss: 79.6633071899414\nEpoch: 0, Generator Loss: -67.60935974121094, Discriminator Loss: 73.61372375488281\nEpoch: 0, Generator Loss: -66.13408660888672, Discriminator Loss: 72.17677307128906\nEpoch: 0, Generator Loss: -52.445838928222656, Discriminator Loss: 58.06557846069336\nEpoch: 0, Generator Loss: -42.31278991699219, Discriminator Loss: 47.40928649902344\nEpoch: 0, Generator Loss: -30.399215698242188, Discriminator Loss: 35.23478317260742\nEpoch: 0, Generator Loss: -18.514144897460938, Discriminator Loss: 23.0023193359375\nEpoch: 0, Generator Loss: 13.368420600891113, Discriminator Loss: -9.19992733001709\nEpoch: 0, Generator Loss: 32.52217102050781, Discriminator Loss: -28.594480514526367\nEpoch: 0, Generator Loss: 52.133670806884766, Discriminator Loss: -48.717533111572266\nEpoch: 0, Generator Loss: 83.2466049194336, Discriminator Loss: -79.8427734375\nEpoch: 0, Generator Loss: 119.41622924804688, Discriminator Loss: -116.28970336914062\nEpoch: 0, Generator Loss: 153.99534606933594, Discriminator Loss: -151.39779663085938\nEpoch: 0, Generator Loss: 185.34715270996094, Discriminator Loss: -183.12710571289062\nEpoch: 0, Generator Loss: 204.4550323486328, Discriminator Loss: -202.71432495117188\nEpoch: 0, Generator Loss: 233.78033447265625, Discriminator Loss: -232.0883331298828\nEpoch: 0, Generator Loss: 250.38931274414062, Discriminator Loss: -248.8968505859375\nEpoch: 0, Generator Loss: 268.02557373046875, Discriminator Loss: -266.9857482910156\nEpoch: 0, Generator Loss: 274.0816650390625, Discriminator Loss: -273.35772705078125\nEpoch: 0, Generator Loss: 282.60418701171875, Discriminator Loss: -282.3980407714844\nEpoch: 0, Generator Loss: 284.1208801269531, Discriminator Loss: -284.0794372558594\nEpoch: 0, Generator Loss: 258.7149963378906, Discriminator Loss: -259.0458984375\nEpoch: 0, Generator Loss: 257.9189147949219, Discriminator Loss: -258.60784912109375\nEpoch: 0, Generator Loss: 238.2891082763672, Discriminator Loss: -239.3345184326172\nEpoch: 0, Generator Loss: 213.66912841796875, Discriminator Loss: -214.74461364746094\nEpoch: 0, Generator Loss: 191.83154296875, Discriminator Loss: -193.32388305664062\nEpoch: 0, Generator Loss: 172.16677856445312, Discriminator Loss: -173.9463348388672\nEpoch: 0, Generator Loss: 154.4382781982422, Discriminator Loss: -156.57403564453125\nEpoch: 0, Generator Loss: 133.02700805664062, Discriminator Loss: -135.58009338378906\nEpoch: 0, Generator Loss: 99.89375305175781, Discriminator Loss: -102.42910766601562\nEpoch: 0, Generator Loss: 84.49458312988281, Discriminator Loss: -87.46502685546875\nEpoch: 0, Generator Loss: 62.83433532714844, Discriminator Loss: -65.96112060546875\nEpoch: 0, Generator Loss: 44.63788604736328, Discriminator Loss: -48.309349060058594\nEpoch: 0, Generator Loss: 27.912246704101562, Discriminator Loss: -31.348297119140625\nEpoch: 0, Generator Loss: 15.204110145568848, Discriminator Loss: -18.891887664794922\nEpoch: 0, Generator Loss: 6.753571033477783, Discriminator Loss: -10.97932243347168\nEpoch: 0, Generator Loss: 1.7581535577774048, Discriminator Loss: -5.984956741333008\nEpoch: 0, Generator Loss: -2.220580816268921, Discriminator Loss: -2.120468854904175\nEpoch: 0, Generator Loss: -5.166784286499023, Discriminator Loss: 0.6778512001037598\nEpoch: 0, Generator Loss: -8.465254783630371, Discriminator Loss: 3.6821961402893066\nEpoch: 0, Generator Loss: -13.986915588378906, Discriminator Loss: 9.133663177490234\nEpoch: 0, Generator Loss: -19.617136001586914, Discriminator Loss: 14.747129440307617\nEpoch: 0, Generator Loss: -23.489582061767578, Discriminator Loss: 17.865827560424805\nEpoch: 0, Generator Loss: -29.641054153442383, Discriminator Loss: 23.999605178833008\nEpoch: 0, Generator Loss: -38.81141662597656, Discriminator Loss: 33.17606735229492\nEpoch: 0, Generator Loss: -44.56865692138672, Discriminator Loss: 39.203948974609375\nEpoch: 0, Generator Loss: -56.22523498535156, Discriminator Loss: 50.29425811767578\nEpoch: 0, Generator Loss: -61.3620491027832, Discriminator Loss: 55.687931060791016\nEpoch: 0, Generator Loss: -71.75165557861328, Discriminator Loss: 66.1793441772461\nEpoch: 0, Generator Loss: -79.0419692993164, Discriminator Loss: 73.26712036132812\nEpoch: 0, Generator Loss: -84.14320373535156, Discriminator Loss: 78.8440170288086\nEpoch: 0, Generator Loss: -98.02500915527344, Discriminator Loss: 92.84882354736328\nEpoch: 0, Generator Loss: -101.46243286132812, Discriminator Loss: 95.98545837402344\nEpoch: 0, Generator Loss: -115.91730499267578, Discriminator Loss: 111.2152099609375\nEpoch: 0, Generator Loss: -115.95516967773438, Discriminator Loss: 111.04178619384766\nEpoch: 0, Generator Loss: -118.11651611328125, Discriminator Loss: 113.38192749023438\nEpoch: 0, Generator Loss: -108.54048919677734, Discriminator Loss: 104.35804748535156\nEpoch: 0, Generator Loss: -123.94181823730469, Discriminator Loss: 119.71546173095703\nEpoch: 0, Generator Loss: -108.5488510131836, Discriminator Loss: 104.6469497680664\nEpoch: 0, Generator Loss: -102.21202087402344, Discriminator Loss: 98.47032165527344\nEpoch: 0, Generator Loss: -91.0654525756836, Discriminator Loss: 87.61900329589844\nEpoch: 0, Generator Loss: -66.59385681152344, Discriminator Loss: 63.324771881103516\nEpoch: 0, Generator Loss: -35.95939636230469, Discriminator Loss: 32.70528030395508\nEpoch: 0, Generator Loss: 11.94226360321045, Discriminator Loss: -14.558235168457031\nEpoch: 0, Generator Loss: 32.455474853515625, Discriminator Loss: -34.87733840942383\nEpoch: 0, Generator Loss: 89.63359069824219, Discriminator Loss: -91.4338607788086\nEpoch: 0, Generator Loss: 158.96868896484375, Discriminator Loss: -160.69314575195312\nEpoch: 0, Generator Loss: 270.5854797363281, Discriminator Loss: -272.0304260253906\nEpoch: 0, Generator Loss: 284.27325439453125, Discriminator Loss: -285.39654541015625\nEpoch: 0, Generator Loss: 332.747314453125, Discriminator Loss: -333.38018798828125\nEpoch: 0, Generator Loss: 380.14410400390625, Discriminator Loss: -380.3997497558594\nEpoch: 0, Generator Loss: 423.4071044921875, Discriminator Loss: -422.8899230957031\nEpoch: 0, Generator Loss: 493.0897521972656, Discriminator Loss: -491.9537353515625\nEpoch: 0, Generator Loss: 511.4526672363281, Discriminator Loss: -510.0777893066406\nEpoch: 0, Generator Loss: 499.71343994140625, Discriminator Loss: -497.8518371582031\nEpoch: 0, Generator Loss: 521.07275390625, Discriminator Loss: -518.6536254882812\nEpoch: 0, Generator Loss: 471.12164306640625, Discriminator Loss: -467.9087829589844\nEpoch: 0, Generator Loss: 476.92218017578125, Discriminator Loss: -473.3879089355469\nEpoch: 0, Generator Loss: 430.3543701171875, Discriminator Loss: -425.95758056640625\nEpoch: 0, Generator Loss: 391.9559326171875, Discriminator Loss: -386.9745178222656\nEpoch: 0, Generator Loss: 351.2749938964844, Discriminator Loss: -345.7976989746094\nEpoch: 0, Generator Loss: 299.5464782714844, Discriminator Loss: -293.366455078125\nEpoch: 0, Generator Loss: 247.2979278564453, Discriminator Loss: -240.207763671875\nEpoch: 0, Generator Loss: 189.49203491210938, Discriminator Loss: -181.8919677734375\nEpoch: 0, Generator Loss: 148.9818115234375, Discriminator Loss: -140.90771484375\nEpoch: 0, Generator Loss: 105.39421844482422, Discriminator Loss: -96.57252502441406\nEpoch: 0, Generator Loss: 70.18587493896484, Discriminator Loss: -60.654510498046875\nEpoch: 0, Generator Loss: 38.47212219238281, Discriminator Loss: -28.638010025024414\nEpoch: 0, Generator Loss: 18.018341064453125, Discriminator Loss: -7.766261100769043\nEpoch: 0, Generator Loss: 7.266458034515381, Discriminator Loss: 3.3174166679382324\nEpoch: 0, Generator Loss: 2.9988794326782227, Discriminator Loss: 8.514914512634277\nEpoch: 0, Generator Loss: 3.957153081893921, Discriminator Loss: 7.770997047424316\nEpoch: 0, Generator Loss: 6.317156791687012, Discriminator Loss: 5.854656219482422\nEpoch: 0, Generator Loss: 9.381616592407227, Discriminator Loss: 2.396120071411133\nEpoch: 0, Generator Loss: 10.638269424438477, Discriminator Loss: 1.8475360870361328\nEpoch: 1, Generator Loss: 12.601343154907227, Discriminator Loss: 0.1645984649658203\nEpoch: 1, Generator Loss: 12.973455429077148, Discriminator Loss: -0.1599864959716797\nEpoch: 1, Generator Loss: 12.019556045532227, Discriminator Loss: 0.8274765014648438\nEpoch: 1, Generator Loss: 11.806975364685059, Discriminator Loss: 1.1024150848388672\nEpoch: 1, Generator Loss: 9.58702278137207, Discriminator Loss: 3.559872627258301\nEpoch: 1, Generator Loss: 9.539985656738281, Discriminator Loss: 3.666166305541992\nEpoch: 1, Generator Loss: 8.216650009155273, Discriminator Loss: 4.89334774017334\nEpoch: 1, Generator Loss: 6.492573261260986, Discriminator Loss: 7.064681529998779\nEpoch: 1, Generator Loss: 4.6034626960754395, Discriminator Loss: 7.929088115692139\nEpoch: 1, Generator Loss: 3.522498369216919, Discriminator Loss: 9.896214485168457\nEpoch: 1, Generator Loss: 2.279539108276367, Discriminator Loss: 10.726672172546387\nEpoch: 1, Generator Loss: 1.022916316986084, Discriminator Loss: 12.088512420654297\nEpoch: 1, Generator Loss: -0.734313428401947, Discriminator Loss: 14.14869213104248\nEpoch: 1, Generator Loss: -3.529475212097168, Discriminator Loss: 16.29810905456543\nEpoch: 1, Generator Loss: -3.9791994094848633, Discriminator Loss: 16.858274459838867\nEpoch: 1, Generator Loss: -6.213697910308838, Discriminator Loss: 18.849445343017578\nEpoch: 1, Generator Loss: -7.112697124481201, Discriminator Loss: 20.204784393310547\nEpoch: 1, Generator Loss: -6.4275994300842285, Discriminator Loss: 19.281015396118164\nEpoch: 1, Generator Loss: -9.380398750305176, Discriminator Loss: 22.703271865844727\nEpoch: 1, Generator Loss: -8.28691577911377, Discriminator Loss: 21.256919860839844\nEpoch: 1, Generator Loss: -9.441119194030762, Discriminator Loss: 21.698638916015625\nEpoch: 1, Generator Loss: -10.51678466796875, Discriminator Loss: 22.337566375732422\nEpoch: 1, Generator Loss: -10.059357643127441, Discriminator Loss: 22.137073516845703\nEpoch: 1, Generator Loss: -11.629866600036621, Discriminator Loss: 22.915510177612305\nEpoch: 1, Generator Loss: -11.559627532958984, Discriminator Loss: 23.480506896972656\nEpoch: 1, Generator Loss: -10.491752624511719, Discriminator Loss: 22.734840393066406\nEpoch: 1, Generator Loss: -11.724517822265625, Discriminator Loss: 22.80137825012207\nEpoch: 1, Generator Loss: -12.946816444396973, Discriminator Loss: 24.455612182617188\nEpoch: 1, Generator Loss: -10.407437324523926, Discriminator Loss: 21.560070037841797\nEpoch: 1, Generator Loss: -12.815181732177734, Discriminator Loss: 23.713729858398438\nEpoch: 1, Generator Loss: -10.82419204711914, Discriminator Loss: 21.468116760253906\nEpoch: 1, Generator Loss: -11.143765449523926, Discriminator Loss: 22.551677703857422\nEpoch: 1, Generator Loss: -10.485713958740234, Discriminator Loss: 21.73148536682129\nEpoch: 1, Generator Loss: -10.117501258850098, Discriminator Loss: 20.289392471313477\nEpoch: 1, Generator Loss: -8.226876258850098, Discriminator Loss: 18.804691314697266\nEpoch: 1, Generator Loss: -8.46292495727539, Discriminator Loss: 18.146690368652344\nEpoch: 1, Generator Loss: -9.906359672546387, Discriminator Loss: 19.96175765991211\nEpoch: 1, Generator Loss: -7.169093132019043, Discriminator Loss: 17.10061264038086\nEpoch: 1, Generator Loss: -4.238447189331055, Discriminator Loss: 13.999754905700684\nEpoch: 1, Generator Loss: -4.073104381561279, Discriminator Loss: 14.229684829711914\nEpoch: 1, Generator Loss: -3.1754441261291504, Discriminator Loss: 13.320966720581055\nEpoch: 1, Generator Loss: -0.5844177007675171, Discriminator Loss: 10.414308547973633\nEpoch: 1, Generator Loss: -1.17124605178833, Discriminator Loss: 10.61585807800293\nEpoch: 1, Generator Loss: 0.03255578875541687, Discriminator Loss: 9.426678657531738\nEpoch: 1, Generator Loss: 3.867190361022949, Discriminator Loss: 5.726978302001953\nEpoch: 1, Generator Loss: 7.502850532531738, Discriminator Loss: 1.720820426940918\nEpoch: 1, Generator Loss: 8.937376976013184, Discriminator Loss: 0.0520172119140625\nEpoch: 1, Generator Loss: 7.267189979553223, Discriminator Loss: 2.508101463317871\nEpoch: 1, Generator Loss: 14.070893287658691, Discriminator Loss: -5.123534202575684\nEpoch: 1, Generator Loss: 14.213603973388672, Discriminator Loss: -5.237910270690918\nEpoch: 1, Generator Loss: 17.47350311279297, Discriminator Loss: -8.03968334197998\nEpoch: 1, Generator Loss: 17.62294578552246, Discriminator Loss: -8.79598331451416\nEpoch: 1, Generator Loss: 23.287355422973633, Discriminator Loss: -14.083707809448242\nEpoch: 1, Generator Loss: 23.0986385345459, Discriminator Loss: -14.035723686218262\nEpoch: 1, Generator Loss: 25.114377975463867, Discriminator Loss: -16.128604888916016\nEpoch: 1, Generator Loss: 29.042688369750977, Discriminator Loss: -20.647062301635742\nEpoch: 1, Generator Loss: 28.883014678955078, Discriminator Loss: -19.979272842407227\nEpoch: 1, Generator Loss: 34.263038635253906, Discriminator Loss: -25.798110961914062\nEpoch: 1, Generator Loss: 33.770790100097656, Discriminator Loss: -25.03310775756836\nEpoch: 1, Generator Loss: 31.6925106048584, Discriminator Loss: -23.32919692993164\nEpoch: 1, Generator Loss: 37.514278411865234, Discriminator Loss: -29.28054428100586\nEpoch: 1, Generator Loss: 39.000770568847656, Discriminator Loss: -29.97345733642578\nEpoch: 1, Generator Loss: 36.42145919799805, Discriminator Loss: -28.299942016601562\nEpoch: 1, Generator Loss: 39.36687469482422, Discriminator Loss: -30.24183464050293\nEpoch: 1, Generator Loss: 36.381011962890625, Discriminator Loss: -27.882251739501953\nEpoch: 1, Generator Loss: 40.22895812988281, Discriminator Loss: -32.27458190917969\nEpoch: 1, Generator Loss: 38.847286224365234, Discriminator Loss: -30.695775985717773\nEpoch: 1, Generator Loss: 36.37309265136719, Discriminator Loss: -28.116680145263672\nEpoch: 1, Generator Loss: 37.526634216308594, Discriminator Loss: -29.236217498779297\nEpoch: 1, Generator Loss: 35.00505065917969, Discriminator Loss: -26.967836380004883\nEpoch: 1, Generator Loss: 35.01008605957031, Discriminator Loss: -26.39618492126465\nEpoch: 1, Generator Loss: 33.69990158081055, Discriminator Loss: -25.413928985595703\nEpoch: 1, Generator Loss: 31.464651107788086, Discriminator Loss: -23.220012664794922\nEpoch: 1, Generator Loss: 30.469316482543945, Discriminator Loss: -21.90058135986328\nEpoch: 1, Generator Loss: 27.552453994750977, Discriminator Loss: -19.180885314941406\nEpoch: 1, Generator Loss: 25.520673751831055, Discriminator Loss: -17.450096130371094\nEpoch: 1, Generator Loss: 21.817466735839844, Discriminator Loss: -13.40173053741455\nEpoch: 1, Generator Loss: 21.493677139282227, Discriminator Loss: -13.223213195800781\nEpoch: 1, Generator Loss: 21.09500503540039, Discriminator Loss: -13.127359390258789\nEpoch: 1, Generator Loss: 17.719499588012695, Discriminator Loss: -9.490766525268555\nEpoch: 1, Generator Loss: 17.102685928344727, Discriminator Loss: -9.232477188110352\nEpoch: 1, Generator Loss: 14.650369644165039, Discriminator Loss: -6.263779640197754\nEpoch: 1, Generator Loss: 12.677715301513672, Discriminator Loss: -5.356921672821045\nEpoch: 1, Generator Loss: 10.892594337463379, Discriminator Loss: -3.39286470413208\nEpoch: 1, Generator Loss: 9.65542221069336, Discriminator Loss: -1.9611883163452148\nEpoch: 1, Generator Loss: 7.56132698059082, Discriminator Loss: 0.11181163787841797\nEpoch: 1, Generator Loss: 6.394404888153076, Discriminator Loss: 0.8554277420043945\nEpoch: 1, Generator Loss: 5.03525447845459, Discriminator Loss: 2.1812920570373535\nEpoch: 1, Generator Loss: 4.601007461547852, Discriminator Loss: 2.485893726348877\nEpoch: 1, Generator Loss: 3.977710008621216, Discriminator Loss: 3.08076548576355\nEpoch: 1, Generator Loss: 3.4099502563476562, Discriminator Loss: 3.742927074432373\nEpoch: 1, Generator Loss: 2.787632465362549, Discriminator Loss: 4.284451961517334\nEpoch: 1, Generator Loss: 2.286874532699585, Discriminator Loss: 4.781574249267578\nEpoch: 1, Generator Loss: 1.8324191570281982, Discriminator Loss: 4.544277191162109\nEpoch: 1, Generator Loss: 1.5822672843933105, Discriminator Loss: 4.892162799835205\nEpoch: 1, Generator Loss: 1.8066891431808472, Discriminator Loss: 5.053703784942627\nEpoch: 1, Generator Loss: 1.5203399658203125, Discriminator Loss: 4.603918075561523\nEpoch: 1, Generator Loss: 1.583364725112915, Discriminator Loss: 5.14213752746582\nEpoch: 1, Generator Loss: 1.5411759614944458, Discriminator Loss: 4.9971809387207\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n.262838363647461\nEpoch: 98, Generator Loss: 22.27802085876465, Discriminator Loss: 0.8649654388427734\nEpoch: 98, Generator Loss: 24.302894592285156, Discriminator Loss: -2.047273635864258\nEpoch: 98, Generator Loss: 32.42487716674805, Discriminator Loss: 2.904979705810547\nEpoch: 98, Generator Loss: 28.187389373779297, Discriminator Loss: -2.148345947265625\nEpoch: 98, Generator Loss: 24.952302932739258, Discriminator Loss: 10.166437149047852\nEpoch: 98, Generator Loss: 31.81359100341797, Discriminator Loss: -0.18932342529296875\nEpoch: 98, Generator Loss: 25.592735290527344, Discriminator Loss: 7.321743011474609\nEpoch: 98, Generator Loss: 30.897184371948242, Discriminator Loss: -13.15941047668457\nEpoch: 98, Generator Loss: 26.785072326660156, Discriminator Loss: -3.9026870727539062\nEpoch: 98, Generator Loss: 28.148754119873047, Discriminator Loss: -11.830209732055664\nEpoch: 98, Generator Loss: 21.31171417236328, Discriminator Loss: 11.002674102783203\nEpoch: 98, Generator Loss: 21.69448471069336, Discriminator Loss: 9.708948135375977\nEpoch: 98, Generator Loss: 16.157554626464844, Discriminator Loss: 8.465564727783203\nEpoch: 98, Generator Loss: 29.09796905517578, Discriminator Loss: -11.54092025756836\nEpoch: 98, Generator Loss: 17.100486755371094, Discriminator Loss: 10.253856658935547\nEpoch: 98, Generator Loss: 25.637897491455078, Discriminator Loss: -6.834650039672852\nEpoch: 98, Generator Loss: 30.94062042236328, Discriminator Loss: 6.2749786376953125\nEpoch: 98, Generator Loss: 26.846084594726562, Discriminator Loss: 0.42344093322753906\nEpoch: 98, Generator Loss: 26.215957641601562, Discriminator Loss: -8.30941390991211\nEpoch: 98, Generator Loss: 17.50946807861328, Discriminator Loss: 4.69865608215332\nEpoch: 98, Generator Loss: 26.030475616455078, Discriminator Loss: 6.3102874755859375\nEpoch: 98, Generator Loss: 15.042367935180664, Discriminator Loss: 6.939167022705078\nEpoch: 98, Generator Loss: 27.747331619262695, Discriminator Loss: 16.57606315612793\nEpoch: 98, Generator Loss: 21.203535079956055, Discriminator Loss: -1.4802837371826172\nEpoch: 98, Generator Loss: 15.518610000610352, Discriminator Loss: -1.201920509338379\nEpoch: 98, Generator Loss: 18.8836727142334, Discriminator Loss: 23.0128116607666\nEpoch: 98, Generator Loss: 27.642578125, Discriminator Loss: -3.8272037506103516\nEpoch: 98, Generator Loss: 11.441876411437988, Discriminator Loss: 13.675667762756348\nEpoch: 98, Generator Loss: 27.821670532226562, Discriminator Loss: 12.757514953613281\nEpoch: 98, Generator Loss: 17.26280403137207, Discriminator Loss: 6.166528701782227\nEpoch: 98, Generator Loss: 25.126985549926758, Discriminator Loss: -1.4291210174560547\nEpoch: 98, Generator Loss: 27.76235580444336, Discriminator Loss: -4.709016799926758\nEpoch: 98, Generator Loss: 24.404373168945312, Discriminator Loss: 6.046377182006836\nEpoch: 98, Generator Loss: 25.26571273803711, Discriminator Loss: -1.4814453125\nEpoch: 98, Generator Loss: 9.830572128295898, Discriminator Loss: 18.831560134887695\nEpoch: 98, Generator Loss: 24.523569107055664, Discriminator Loss: 1.4736137390136719\nEpoch: 98, Generator Loss: 21.273681640625, Discriminator Loss: -8.944366455078125\nEpoch: 98, Generator Loss: 17.96807098388672, Discriminator Loss: 0.6030521392822266\nEpoch: 98, Generator Loss: 20.39179801940918, Discriminator Loss: 1.636575698852539\nEpoch: 98, Generator Loss: 23.38763427734375, Discriminator Loss: -4.134151458740234\nEpoch: 98, Generator Loss: 16.414472579956055, Discriminator Loss: 7.877758026123047\nEpoch: 98, Generator Loss: 23.435693740844727, Discriminator Loss: 4.838544845581055\nEpoch: 98, Generator Loss: 19.129714965820312, Discriminator Loss: 6.788888931274414\nEpoch: 98, Generator Loss: 25.36827850341797, Discriminator Loss: -5.260660171508789\nEpoch: 98, Generator Loss: 17.79230308532715, Discriminator Loss: 21.548402786254883\nEpoch: 98, Generator Loss: 29.98332977294922, Discriminator Loss: -3.492198944091797\nEpoch: 98, Generator Loss: 7.94041633605957, Discriminator Loss: 17.462833404541016\nEpoch: 98, Generator Loss: 15.867841720581055, Discriminator Loss: 6.120319366455078\nEpoch: 98, Generator Loss: 24.205717086791992, Discriminator Loss: -8.403038024902344\nEpoch: 98, Generator Loss: 12.31801986694336, Discriminator Loss: 6.607280731201172\nEpoch: 98, Generator Loss: 27.4373779296875, Discriminator Loss: -2.919157028198242\nEpoch: 98, Generator Loss: 14.298698425292969, Discriminator Loss: 14.74881362915039\nEpoch: 98, Generator Loss: 14.42293930053711, Discriminator Loss: 2.1046810150146484\nEpoch: 98, Generator Loss: 11.863149642944336, Discriminator Loss: -0.0520172119140625\nEpoch: 98, Generator Loss: 33.635501861572266, Discriminator Loss: -16.27273941040039\nEpoch: 98, Generator Loss: 34.729732513427734, Discriminator Loss: -4.548191070556641\nEpoch: 98, Generator Loss: 30.507722854614258, Discriminator Loss: -6.585212707519531\nEpoch: 98, Generator Loss: 30.646102905273438, Discriminator Loss: -4.195579528808594\nEpoch: 98, Generator Loss: 27.693801879882812, Discriminator Loss: -19.115585327148438\nEpoch: 98, Generator Loss: 21.47112464904785, Discriminator Loss: 8.501724243164062\nEpoch: 98, Generator Loss: 24.604841232299805, Discriminator Loss: -5.932537078857422\nEpoch: 98, Generator Loss: 29.965431213378906, Discriminator Loss: -4.144201278686523\nEpoch: 98, Generator Loss: 41.9844970703125, Discriminator Loss: -10.189325332641602\nEpoch: 98, Generator Loss: 22.441417694091797, Discriminator Loss: 13.959976196289062\nEpoch: 98, Generator Loss: 31.082313537597656, Discriminator Loss: -0.5555381774902344\nEpoch: 98, Generator Loss: 30.652050018310547, Discriminator Loss: -3.631847381591797\nEpoch: 98, Generator Loss: 35.59119415283203, Discriminator Loss: -0.5813713073730469\nEpoch: 98, Generator Loss: 40.6495475769043, Discriminator Loss: -18.498380661010742\nEpoch: 98, Generator Loss: 32.533512115478516, Discriminator Loss: 6.468227386474609\nEpoch: 98, Generator Loss: 30.491352081298828, Discriminator Loss: -7.183238983154297\nEpoch: 98, Generator Loss: 29.567852020263672, Discriminator Loss: -16.035903930664062\nEpoch: 98, Generator Loss: 32.3045539855957, Discriminator Loss: -5.25677490234375\nEpoch: 98, Generator Loss: 31.4820499420166, Discriminator Loss: -11.546846389770508\nEpoch: 98, Generator Loss: 37.732032775878906, Discriminator Loss: -18.235929489135742\nEpoch: 98, Generator Loss: 39.48821258544922, Discriminator Loss: -15.901113510131836\nEpoch: 98, Generator Loss: 35.00897216796875, Discriminator Loss: -7.756507873535156\nEpoch: 98, Generator Loss: 35.14347839355469, Discriminator Loss: -5.565385818481445\nEpoch: 98, Generator Loss: 27.113746643066406, Discriminator Loss: 3.4668655395507812\nEpoch: 98, Generator Loss: 35.97603988647461, Discriminator Loss: -14.615655899047852\nEpoch: 98, Generator Loss: 29.340011596679688, Discriminator Loss: 2.740795135498047\nEpoch: 98, Generator Loss: 36.047576904296875, Discriminator Loss: -5.427938461303711\nEpoch: 98, Generator Loss: 42.133426666259766, Discriminator Loss: -9.046802520751953\nEpoch: 98, Generator Loss: 37.20334243774414, Discriminator Loss: -9.560291290283203\nEpoch: 98, Generator Loss: 42.342342376708984, Discriminator Loss: -7.719322204589844\nEpoch: 98, Generator Loss: 29.660715103149414, Discriminator Loss: -14.844290733337402\nEpoch: 98, Generator Loss: 23.444360733032227, Discriminator Loss: 12.15995979309082\nEpoch: 98, Generator Loss: 35.02021408081055, Discriminator Loss: -1.3769760131835938\nEpoch: 98, Generator Loss: 35.161991119384766, Discriminator Loss: 0.19530105590820312\nEpoch: 98, Generator Loss: 39.11054611206055, Discriminator Loss: -6.567535400390625\nEpoch: 98, Generator Loss: 31.407873153686523, Discriminator Loss: -0.660888671875\nEpoch: 98, Generator Loss: 34.609859466552734, Discriminator Loss: -7.638622283935547\nEpoch: 98, Generator Loss: 37.173133850097656, Discriminator Loss: -0.3721466064453125\nEpoch: 98, Generator Loss: 45.34355545043945, Discriminator Loss: -18.533185958862305\nEpoch: 98, Generator Loss: 26.428573608398438, Discriminator Loss: 8.48392105102539\nEpoch: 98, Generator Loss: 29.745656967163086, Discriminator Loss: 2.1630935668945312\nEpoch: 98, Generator Loss: 27.519119262695312, Discriminator Loss: -0.08114242553710938\nEpoch: 98, Generator Loss: 31.234540939331055, Discriminator Loss: 0.6350536346435547\nEpoch: 99, Generator Loss: 35.97956848144531, Discriminator Loss: -16.846446990966797\nEpoch: 99, Generator Loss: 24.450756072998047, Discriminator Loss: 9.456912994384766\nEpoch: 99, Generator Loss: 29.952112197875977, Discriminator Loss: -6.247873306274414\nEpoch: 99, Generator Loss: 28.1842098236084, Discriminator Loss: -0.9226474761962891\nEpoch: 99, Generator Loss: 30.134557723999023, Discriminator Loss: -4.323392868041992\nEpoch: 99, Generator Loss: 20.129554748535156, Discriminator Loss: 6.282867431640625\nEpoch: 99, Generator Loss: 24.52356719970703, Discriminator Loss: 7.4246063232421875\nEpoch: 99, Generator Loss: 33.659019470214844, Discriminator Loss: 1.4211654663085938\nEpoch: 99, Generator Loss: 14.615677833557129, Discriminator Loss: 11.641438484191895\nEpoch: 99, Generator Loss: 20.63105010986328, Discriminator Loss: 9.414087295532227\nEpoch: 99, Generator Loss: 15.584253311157227, Discriminator Loss: 7.354503631591797\nEpoch: 99, Generator Loss: 32.0586051940918, Discriminator Loss: 1.0602340698242188\nEpoch: 99, Generator Loss: 27.204465866088867, Discriminator Loss: 15.227483749389648\nEpoch: 99, Generator Loss: 30.56305694580078, Discriminator Loss: -9.66175651550293\nEpoch: 99, Generator Loss: 30.201383590698242, Discriminator Loss: -3.4462547302246094\nEpoch: 99, Generator Loss: 28.032978057861328, Discriminator Loss: -2.757183074951172\nEpoch: 99, Generator Loss: 25.8680362701416, Discriminator Loss: 9.791749954223633\nEpoch: 99, Generator Loss: 38.66582107543945, Discriminator Loss: -21.164241790771484\nEpoch: 99, Generator Loss: 32.01970291137695, Discriminator Loss: -8.842952728271484\nEpoch: 99, Generator Loss: 31.008960723876953, Discriminator Loss: 10.178150177001953\nEpoch: 99, Generator Loss: 26.455636978149414, Discriminator Loss: -4.71544075012207\nEpoch: 99, Generator Loss: 32.9975471496582, Discriminator Loss: -3.7052249908447266\nEpoch: 99, Generator Loss: 33.589683532714844, Discriminator Loss: 6.633552551269531\nEpoch: 99, Generator Loss: 35.54414367675781, Discriminator Loss: -0.8467636108398438\nEpoch: 99, Generator Loss: 27.906023025512695, Discriminator Loss: 5.095129013061523\nEpoch: 99, Generator Loss: 39.38616180419922, Discriminator Loss: -18.526103973388672\nEpoch: 99, Generator Loss: 38.922271728515625, Discriminator Loss: -11.878868103027344\nEpoch: 99, Generator Loss: 36.95863723754883, Discriminator Loss: -19.516651153564453\nEpoch: 99, Generator Loss: 26.18283462524414, Discriminator Loss: 4.586757659912109\nEpoch: 99, Generator Loss: 30.840749740600586, Discriminator Loss: -13.105470657348633\nEpoch: 99, Generator Loss: 30.707178115844727, Discriminator Loss: 5.40556526184082\nEpoch: 99, Generator Loss: 28.628807067871094, Discriminator Loss: -0.30541038513183594\nEpoch: 99, Generator Loss: 21.585559844970703, Discriminator Loss: 4.108360290527344\nEpoch: 99, Generator Loss: 25.546119689941406, Discriminator Loss: 7.3277130126953125\nEpoch: 99, Generator Loss: 31.82402801513672, Discriminator Loss: -9.346771240234375\nEpoch: 99, Generator Loss: 22.809978485107422, Discriminator Loss: 0.117523193359375\nEpoch: 99, Generator Loss: 28.997150421142578, Discriminator Loss: -1.8441524505615234\nEpoch: 99, Generator Loss: 27.820083618164062, Discriminator Loss: 1.9590415954589844\nEpoch: 99, Generator Loss: 17.23419952392578, Discriminator Loss: 14.030651092529297\nEpoch: 99, Generator Loss: 23.73740005493164, Discriminator Loss: -1.0696735382080078\nEpoch: 99, Generator Loss: 25.19564437866211, Discriminator Loss: 2.156574249267578\nEpoch: 99, Generator Loss: 17.500741958618164, Discriminator Loss: 14.252632141113281\nEpoch: 99, Generator Loss: 25.065410614013672, Discriminator Loss: 5.164951324462891\nEpoch: 99, Generator Loss: 15.97137451171875, Discriminator Loss: 10.798830032348633\nEpoch: 99, Generator Loss: 19.39755630493164, Discriminator Loss: 7.169139862060547\nEpoch: 99, Generator Loss: 22.402015686035156, Discriminator Loss: 3.0874881744384766\nEpoch: 99, Generator Loss: 7.729124546051025, Discriminator Loss: 11.686286926269531\nEpoch: 99, Generator Loss: 27.428850173950195, Discriminator Loss: -4.301094055175781\nEpoch: 99, Generator Loss: 30.430639266967773, Discriminator Loss: 4.252321243286133\nEpoch: 99, Generator Loss: 27.270689010620117, Discriminator Loss: 5.37324333190918\nEpoch: 99, Generator Loss: 27.738380432128906, Discriminator Loss: 7.588623046875\nEpoch: 99, Generator Loss: 14.431713104248047, Discriminator Loss: 12.360595703125\nEpoch: 99, Generator Loss: 19.815479278564453, Discriminator Loss: 12.865074157714844\nEpoch: 99, Generator Loss: 15.395500183105469, Discriminator Loss: 17.57009506225586\nEpoch: 99, Generator Loss: 29.735010147094727, Discriminator Loss: 6.760541915893555\nEpoch: 99, Generator Loss: 29.45926284790039, Discriminator Loss: -7.206872940063477\nEpoch: 99, Generator Loss: 20.352502822875977, Discriminator Loss: 11.749509811401367\nEpoch: 99, Generator Loss: 42.044837951660156, Discriminator Loss: -7.646366119384766\nEpoch: 99, Generator Loss: 30.913246154785156, Discriminator Loss: -4.911691665649414\nEpoch: 99, Generator Loss: 25.370887756347656, Discriminator Loss: 25.725631713867188\nEpoch: 99, Generator Loss: 20.38500213623047, Discriminator Loss: 14.443668365478516\nEpoch: 99, Generator Loss: 35.58240509033203, Discriminator Loss: -2.9737586975097656\nEpoch: 99, Generator Loss: 19.851722717285156, Discriminator Loss: 11.911626815795898\nEpoch: 99, Generator Loss: 29.711750030517578, Discriminator Loss: 18.48239517211914\nEpoch: 99, Generator Loss: 29.951353073120117, Discriminator Loss: 6.584234237670898\nEpoch: 99, Generator Loss: 33.142555236816406, Discriminator Loss: 10.40414810180664\nEpoch: 99, Generator Loss: 31.95299530029297, Discriminator Loss: 4.32635498046875\nEpoch: 99, Generator Loss: 37.291786193847656, Discriminator Loss: -5.385530471801758\nEpoch: 99, Generator Loss: 25.3861083984375, Discriminator Loss: 2.315664291381836\nEpoch: 99, Generator Loss: 27.4771728515625, Discriminator Loss: 11.488765716552734\nEpoch: 99, Generator Loss: 33.862518310546875, Discriminator Loss: 3.6686553955078125\nEpoch: 99, Generator Loss: 40.99251937866211, Discriminator Loss: 0.4326667785644531\nEpoch: 99, Generator Loss: 43.63935089111328, Discriminator Loss: -13.583250045776367\nEpoch: 99, Generator Loss: 27.747295379638672, Discriminator Loss: 13.97781753540039\nEpoch: 99, Generator Loss: 30.002586364746094, Discriminator Loss: 7.588676452636719\nEpoch: 99, Generator Loss: 40.553466796875, Discriminator Loss: -1.6297035217285156\nEpoch: 99, Generator Loss: 28.396074295043945, Discriminator Loss: 3.046079635620117\nEpoch: 99, Generator Loss: 30.253019332885742, Discriminator Loss: 8.080446243286133\nEpoch: 99, Generator Loss: 44.9281120300293, Discriminator Loss: -11.317737579345703\nEpoch: 99, Generator Loss: 32.550148010253906, Discriminator Loss: 1.27801513671875\nEpoch: 99, Generator Loss: 28.124605178833008, Discriminator Loss: -1.483682632446289\nEpoch: 99, Generator Loss: 34.03292465209961, Discriminator Loss: -0.6211433410644531\nEpoch: 99, Generator Loss: 28.60076332092285, Discriminator Loss: 3.0938167572021484\nEpoch: 99, Generator Loss: 32.72367858886719, Discriminator Loss: 8.09835433959961\nEpoch: 99, Generator Loss: 36.883522033691406, Discriminator Loss: -8.976158142089844\nEpoch: 99, Generator Loss: 40.921241760253906, Discriminator Loss: -3.619384765625\nEpoch: 99, Generator Loss: 33.94468688964844, Discriminator Loss: -3.1886768341064453\nEpoch: 99, Generator Loss: 35.98948287963867, Discriminator Loss: -0.7738037109375\nEpoch: 99, Generator Loss: 36.73583984375, Discriminator Loss: -0.3752708435058594\nEpoch: 99, Generator Loss: 40.178314208984375, Discriminator Loss: -6.006000518798828\nEpoch: 99, Generator Loss: 51.06544876098633, Discriminator Loss: -14.890689849853516\nEpoch: 99, Generator Loss: 46.6138801574707, Discriminator Loss: -10.200637817382812\nEpoch: 99, Generator Loss: 46.66064453125, Discriminator Loss: -17.67258071899414\nEpoch: 99, Generator Loss: 35.76306915283203, Discriminator Loss: -3.5780563354492188\nEpoch: 99, Generator Loss: 40.550804138183594, Discriminator Loss: -15.26923942565918\nEpoch: 99, Generator Loss: 36.4270133972168, Discriminator Loss: -6.882526397705078\nEpoch: 99, Generator Loss: 45.005340576171875, Discriminator Loss: -10.613151550292969\nEpoch: 99, Generator Loss: 51.2291259765625, Discriminator Loss: -20.609207153320312\nEpoch: 99, Generator Loss: 28.826929092407227, Discriminator Loss: 10.642583847045898\nEpoch: 99, Generator Loss: 45.54560470581055, Discriminator Loss: -11.003616333007812\nEpoch: 99, Generator Loss: 43.996490478515625, Discriminator Loss: -14.254762649536133\nEpoch: 99, Generator Loss: 40.6909294128418, Discriminator Loss: -6.654766082763672\nEpoch: 99, Generator Loss: 35.08845901489258, Discriminator Loss: -1.0085868835449219\nEpoch: 99, Generator Loss: 43.02577590942383, Discriminator Loss: -4.812339782714844\nEpoch: 99, Generator Loss: 47.42513656616211, Discriminator Loss: -11.533309936523438\nEpoch: 99, Generator Loss: 40.479373931884766, Discriminator Loss: -7.807201385498047\nEpoch: 99, Generator Loss: 38.85288619995117, Discriminator Loss: -12.212703704833984\nEpoch: 99, Generator Loss: 43.00346374511719, Discriminator Loss: -14.901227951049805\nEpoch: 99, Generator Loss: 44.89772415161133, Discriminator Loss: -19.463581085205078\nEpoch: 99, Generator Loss: 30.32406234741211, Discriminator Loss: -13.577409744262695\nEpoch: 99, Generator Loss: 32.836463928222656, Discriminator Loss: -7.349025726318359\nEpoch: 99, Generator Loss: 39.294700622558594, Discriminator Loss: -10.406288146972656\nEpoch: 99, Generator Loss: 49.569419860839844, Discriminator Loss: -13.649513244628906\nEpoch: 99, Generator Loss: 30.697751998901367, Discriminator Loss: 2.470396041870117\nEpoch: 99, Generator Loss: 31.76690101623535, Discriminator Loss: -7.935731887817383\nEpoch: 99, Generator Loss: 24.20538902282715, Discriminator Loss: 7.532737731933594\nEpoch: 99, Generator Loss: 24.841171264648438, Discriminator Loss: 6.527191162109375\nEpoch: 99, Generator Loss: 29.724050521850586, Discriminator Loss: -0.8681812286376953\nEpoch: 99, Generator Loss: 28.99846839904785, Discriminator Loss: 4.588014602661133\nEpoch: 99, Generator Loss: 25.764066696166992, Discriminator Loss: 11.054975509643555\nEpoch: 99, Generator Loss: 25.174779891967773, Discriminator Loss: 1.5005550384521484\nEpoch: 99, Generator Loss: 20.8570613861084, Discriminator Loss: 11.201906204223633\nEpoch: 99, Generator Loss: 28.24286651611328, Discriminator Loss: -1.014413833618164\nEpoch: 99, Generator Loss: 30.647327423095703, Discriminator Loss: 3.9820175170898438\nEpoch: 99, Generator Loss: 25.556489944458008, Discriminator Loss: 3.1865615844726562\nEpoch: 99, Generator Loss: 21.345693588256836, Discriminator Loss: 2.4599857330322266\nEpoch: 99, Generator Loss: 21.518831253051758, Discriminator Loss: 10.309076309204102\nEpoch: 99, Generator Loss: 34.431800842285156, Discriminator Loss: 1.9832839965820312\nEpoch: 99, Generator Loss: 27.7961368560791, Discriminator Loss: 5.251497268676758\nEpoch: 99, Generator Loss: 21.587989807128906, Discriminator Loss: 10.676593780517578\nEpoch: 99, Generator Loss: 20.628459930419922, Discriminator Loss: 4.741981506347656\nEpoch: 99, Generator Loss: 15.41556167602539, Discriminator Loss: 9.407596588134766\nEpoch: 99, Generator Loss: 30.24462127685547, Discriminator Loss: 2.921154022216797\nEpoch: 99, Generator Loss: 20.223644256591797, Discriminator Loss: 11.09866714477539\nEpoch: 99, Generator Loss: 26.830039978027344, Discriminator Loss: 6.395961761474609\nEpoch: 99, Generator Loss: 27.807523727416992, Discriminator Loss: 9.559953689575195\nEpoch: 99, Generator Loss: 16.17243766784668, Discriminator Loss: 22.527132034301758\nEpoch: 99, Generator Loss: 25.165292739868164, Discriminator Loss: -0.7367477416992188\nEpoch: 99, Generator Loss: 34.88539505004883, Discriminator Loss: 1.5351829528808594\nEpoch: 99, Generator Loss: 22.96420669555664, Discriminator Loss: 3.4157028198242188\nEpoch: 99, Generator Loss: 35.067298889160156, Discriminator Loss: -9.713302612304688\nEpoch: 99, Generator Loss: 26.323902130126953, Discriminator Loss: -1.7104110717773438\nEpoch: 99, Generator Loss: 31.439233779907227, Discriminator Loss: -3.7014598846435547\nEpoch: 99, Generator Loss: 30.986923217773438, Discriminator Loss: -7.127115249633789\nEpoch: 99, Generator Loss: 18.925168991088867, Discriminator Loss: 6.799440383911133\nEpoch: 99, Generator Loss: 33.47691345214844, Discriminator Loss: -8.140497207641602\nEpoch: 99, Generator Loss: 28.657228469848633, Discriminator Loss: -7.134252548217773\nEpoch: 99, Generator Loss: 28.235797882080078, Discriminator Loss: -9.13079833984375\nEpoch: 99, Generator Loss: 24.32233428955078, Discriminator Loss: 3.182147979736328\nEpoch: 99, Generator Loss: 34.3346061706543, Discriminator Loss: -7.582609176635742\nEpoch: 99, Generator Loss: 27.113176345825195, Discriminator Loss: -0.4007072448730469\nEpoch: 99, Generator Loss: 37.394718170166016, Discriminator Loss: -8.213502883911133\nEpoch: 99, Generator Loss: 39.5789680480957, Discriminator Loss: -10.136531829833984\nEpoch: 99, Generator Loss: 33.774574279785156, Discriminator Loss: -8.564493179321289\nEpoch: 99, Generator Loss: 33.74957275390625, Discriminator Loss: -3.452951431274414\nEpoch: 99, Generator Loss: 30.398405075073242, Discriminator Loss: -8.412817001342773\nEpoch: 99, Generator Loss: 25.360857009887695, Discriminator Loss: 12.414621353149414\nEpoch: 99, Generator Loss: 36.4126091003418, Discriminator Loss: -0.13830184936523438\nEpoch: 99, Generator Loss: 29.811012268066406, Discriminator Loss: 0.13464736938476562\nEpoch: 99, Generator Loss: 36.39265060424805, Discriminator Loss: -2.44049072265625\nEpoch: 99, Generator Loss: 28.580774307250977, Discriminator Loss: -0.08766365051269531\nEpoch: 99, Generator Loss: 43.65972137451172, Discriminator Loss: -26.463918685913086\nEpoch: 99, Generator Loss: 40.82303237915039, Discriminator Loss: -5.734161376953125\nEpoch: 99, Generator Loss: 38.89588928222656, Discriminator Loss: -2.6129188537597656\nEpoch: 99, Generator Loss: 39.98853302001953, Discriminator Loss: -4.562904357910156\nEpoch: 99, Generator Loss: 33.55746841430664, Discriminator Loss: -4.268760681152344\nEpoch: 99, Generator Loss: 36.01426696777344, Discriminator Loss: 12.6751708984375\nEpoch: 99, Generator Loss: 41.926143646240234, Discriminator Loss: -13.573358535766602\nEpoch: 99, Generator Loss: 38.80513000488281, Discriminator Loss: 6.088115692138672\nEpoch: 99, Generator Loss: 35.916866302490234, Discriminator Loss: 0.4322052001953125\nEpoch: 99, Generator Loss: 36.89103698730469, Discriminator Loss: 9.814891815185547\nEpoch: 99, Generator Loss: 34.0504264831543, Discriminator Loss: 0.10583877563476562\nEpoch: 99, Generator Loss: 39.22610092163086, Discriminator Loss: -6.334171295166016\nEpoch: 99, Generator Loss: 38.19110870361328, Discriminator Loss: -0.7199058532714844\nEpoch: 99, Generator Loss: 33.095436096191406, Discriminator Loss: 8.926383972167969\nEpoch: 99, Generator Loss: 41.38954162597656, Discriminator Loss: 1.47198486328125\nEpoch: 99, Generator Loss: 26.740093231201172, Discriminator Loss: 17.13489532470703\nEpoch: 99, Generator Loss: 38.820011138916016, Discriminator Loss: 6.881649017333984\nEpoch: 99, Generator Loss: 47.46060562133789, Discriminator Loss: -8.156078338623047\nEpoch: 99, Generator Loss: 50.298683166503906, Discriminator Loss: -9.096702575683594\nEpoch: 99, Generator Loss: 46.709190368652344, Discriminator Loss: -14.917314529418945\nEpoch: 99, Generator Loss: 39.306846618652344, Discriminator Loss: -1.5413131713867188\nEpoch: 99, Generator Loss: 35.766571044921875, Discriminator Loss: 4.59613037109375\nEpoch: 99, Generator Loss: 41.726863861083984, Discriminator Loss: 3.0600929260253906\nEpoch: 99, Generator Loss: 36.77513122558594, Discriminator Loss: 5.946781158447266\nEpoch: 99, Generator Loss: 39.47309875488281, Discriminator Loss: 0.11631393432617188\nEpoch: 99, Generator Loss: 36.14236068725586, Discriminator Loss: 4.109409332275391\nEpoch: 99, Generator Loss: 41.1290283203125, Discriminator Loss: 6.966869354248047\nEpoch: 99, Generator Loss: 49.026817321777344, Discriminator Loss: 1.0954780578613281\nEpoch: 99, Generator Loss: 38.91292953491211, Discriminator Loss: 7.972686767578125\nEpoch: 99, Generator Loss: 37.68598556518555, Discriminator Loss: 9.344013214111328\nEpoch: 99, Generator Loss: 40.217655181884766, Discriminator Loss: 5.4384613037109375\nEpoch: 99, Generator Loss: 38.85478210449219, Discriminator Loss: 4.34375\nEpoch: 99, Generator Loss: 39.960758209228516, Discriminator Loss: 3.085247039794922\nEpoch: 99, Generator Loss: 33.529232025146484, Discriminator Loss: 15.823173522949219\nEpoch: 99, Generator Loss: 35.065704345703125, Discriminator Loss: 15.63119888305664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize lists to store the losses\n",
    "gen_losses = []\n",
    "disc_losses = []\n",
    "for epoch in range(epochs):\n",
    "    for real_data, classes in dataset:  \n",
    "        gen_loss, disc_loss = train_step(real_data, classes, generator, discriminator, generator_optimizer, discriminator_optimizer, batch_size, noise_dim)\n",
    "        \n",
    "        # Log losses after each epoch or batch as per your preference\n",
    "        mlflow.log_metric(\"Generator Loss\", gen_loss, step=epoch)\n",
    "        mlflow.log_metric(\"Discriminator Loss\", disc_loss, step=epoch)\n",
    "        \n",
    "        # Append the losses to the lists\n",
    "        \n",
    "        gen_losses.append(gen_loss)\n",
    "        disc_losses.append(disc_loss)\n",
    "        \n",
    "        # Log the losses or perform any other monitoring step\n",
    "        print(f\"Epoch: {epoch}, Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639b5087-e15d-4737-a1fe-321727403b2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a DataFrame from the losses\n",
    "loss_data = pd.DataFrame({\n",
    "    'Generator Loss': gen_losses,\n",
    "    'Discriminator Loss': disc_losses\n",
    "})\n",
    "# Convert all the tensor values to floats\n",
    "loss_data['Generator Loss'] = loss_data['Generator Loss'].apply(lambda x: x.numpy() if isinstance(x, tf.Tensor) else x)\n",
    "loss_data['Discriminator Loss'] = loss_data['Discriminator Loss'].apply(lambda x: x.numpy() if isinstance(x, tf.Tensor) else x)\n",
    "# Save the DataFrame as a CSV or log it as an artifact in MLflow\n",
    "loss_data.to_csv(\"/dbfs/FileStore/m332479/GANs_forCyberSecurity/loss_df_tf.csv\")\n",
    "mlflow.log_artifact(\"/dbfs/FileStore/m332479/GANs_forCyberSecurity/loss_df_tf.csv\")\n",
    "\n",
    "# Plot the loss.\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "loss_data.plot(ax=ax)\n",
    "plt.title('cGAN Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['G Loss', 'D Loss'])\n",
    "plt.grid(True)\n",
    "\n",
    "# Make sure to use a correct path to save the plot.\n",
    "plot_path =  \"/dbfs/FileStore/m332479/GANs_forCyberSecurity/plots/loss_plot_tf.png\"\n",
    "plt.savefig(plot_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b1aced3-2958-4c50-ae02-47877bf80b41",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Integrating the Training Step: Epoch Level Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df17ea5-851c-4445-9510-428fe2ad713a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Mean Generator Loss: 16.835887908935547, Mean Discriminator Loss: 0.5890781879425049\nEpoch: 1, Mean Generator Loss: 12.778507232666016, Mean Discriminator Loss: -0.4515567719936371\nEpoch: 2, Mean Generator Loss: 22.116884231567383, Mean Discriminator Loss: -1.2316889762878418\nEpoch: 3, Mean Generator Loss: 19.47618293762207, Mean Discriminator Loss: -0.8731077313423157\nEpoch: 4, Mean Generator Loss: 25.691789627075195, Mean Discriminator Loss: -0.5379351377487183\nEpoch: 5, Mean Generator Loss: 17.224470138549805, Mean Discriminator Loss: -0.020078834146261215\nEpoch: 6, Mean Generator Loss: 23.575170516967773, Mean Discriminator Loss: -1.3520432710647583\nEpoch: 7, Mean Generator Loss: 40.300453186035156, Mean Discriminator Loss: -0.7049715518951416\nEpoch: 8, Mean Generator Loss: 11.095524787902832, Mean Discriminator Loss: 0.6349419355392456\nEpoch: 9, Mean Generator Loss: 12.051127433776855, Mean Discriminator Loss: -0.3666296899318695\nEpoch: 10, Mean Generator Loss: 13.506093978881836, Mean Discriminator Loss: -0.5475772619247437\nEpoch: 11, Mean Generator Loss: 20.576805114746094, Mean Discriminator Loss: -1.3134260177612305\nEpoch: 12, Mean Generator Loss: 20.89653968811035, Mean Discriminator Loss: -1.2068897485733032\nEpoch: 13, Mean Generator Loss: 27.839500427246094, Mean Discriminator Loss: 0.2152458131313324\nEpoch: 14, Mean Generator Loss: 26.392173767089844, Mean Discriminator Loss: 0.02316765859723091\nEpoch: 15, Mean Generator Loss: 21.65685272216797, Mean Discriminator Loss: -2.385976791381836\nEpoch: 16, Mean Generator Loss: 24.691335678100586, Mean Discriminator Loss: -0.2129296511411667\nEpoch: 17, Mean Generator Loss: 22.010173797607422, Mean Discriminator Loss: -0.41198715567588806\nEpoch: 18, Mean Generator Loss: 28.65433692932129, Mean Discriminator Loss: 0.3339471220970154\nEpoch: 19, Mean Generator Loss: 31.496978759765625, Mean Discriminator Loss: -1.555148959159851\nEpoch: 20, Mean Generator Loss: 16.98490333557129, Mean Discriminator Loss: -2.2658627033233643\nEpoch: 21, Mean Generator Loss: 20.97391128540039, Mean Discriminator Loss: -3.2484028339385986\nEpoch: 22, Mean Generator Loss: 28.985576629638672, Mean Discriminator Loss: -1.355597972869873\nEpoch: 23, Mean Generator Loss: 26.39400291442871, Mean Discriminator Loss: 0.2124660313129425\nEpoch: 24, Mean Generator Loss: 39.555912017822266, Mean Discriminator Loss: 0.43065381050109863\nEpoch: 25, Mean Generator Loss: 19.111289978027344, Mean Discriminator Loss: -1.192104458808899\nEpoch: 26, Mean Generator Loss: 23.460100173950195, Mean Discriminator Loss: -1.3358839750289917\nEpoch: 27, Mean Generator Loss: 15.788422584533691, Mean Discriminator Loss: -1.1243735551834106\nEpoch: 28, Mean Generator Loss: 17.595102310180664, Mean Discriminator Loss: 3.2479360103607178\nEpoch: 29, Mean Generator Loss: 20.834552764892578, Mean Discriminator Loss: -1.6956512928009033\nEpoch: 30, Mean Generator Loss: 29.616952896118164, Mean Discriminator Loss: -1.8913377523422241\nEpoch: 31, Mean Generator Loss: 26.002107620239258, Mean Discriminator Loss: -1.4236751794815063\nEpoch: 32, Mean Generator Loss: 31.997493743896484, Mean Discriminator Loss: -1.2109454870224\nEpoch: 33, Mean Generator Loss: 32.00275421142578, Mean Discriminator Loss: -2.11025071144104\nEpoch: 34, Mean Generator Loss: 31.392284393310547, Mean Discriminator Loss: -2.069093704223633\nEpoch: 35, Mean Generator Loss: 44.647945404052734, Mean Discriminator Loss: 0.15702153742313385\nEpoch: 36, Mean Generator Loss: 16.924230575561523, Mean Discriminator Loss: -1.5772380828857422\nEpoch: 37, Mean Generator Loss: 42.30188751220703, Mean Discriminator Loss: -1.860251545906067\nEpoch: 38, Mean Generator Loss: 28.774240493774414, Mean Discriminator Loss: 0.011731595732271671\nEpoch: 39, Mean Generator Loss: 15.493718147277832, Mean Discriminator Loss: -2.4499707221984863\nEpoch: 40, Mean Generator Loss: 24.114402770996094, Mean Discriminator Loss: 1.566800832748413\nEpoch: 41, Mean Generator Loss: 56.1578254699707, Mean Discriminator Loss: -1.0874011516571045\nEpoch: 42, Mean Generator Loss: 33.928672790527344, Mean Discriminator Loss: -1.1541109085083008\nEpoch: 43, Mean Generator Loss: 16.217578887939453, Mean Discriminator Loss: 1.6044200658798218\nEpoch: 44, Mean Generator Loss: 13.510978698730469, Mean Discriminator Loss: -1.1937540769577026\nEpoch: 45, Mean Generator Loss: 9.983745574951172, Mean Discriminator Loss: 1.1603519916534424\nEpoch: 46, Mean Generator Loss: 32.42747116088867, Mean Discriminator Loss: -1.7255362272262573\nEpoch: 47, Mean Generator Loss: 16.764089584350586, Mean Discriminator Loss: -0.7299755811691284\nEpoch: 48, Mean Generator Loss: 26.103595733642578, Mean Discriminator Loss: -1.9104070663452148\nEpoch: 49, Mean Generator Loss: 28.249887466430664, Mean Discriminator Loss: -2.7913734912872314\nEpoch: 50, Mean Generator Loss: 12.007401466369629, Mean Discriminator Loss: 0.47333860397338867\nEpoch: 51, Mean Generator Loss: 40.97564697265625, Mean Discriminator Loss: -0.6419749855995178\nEpoch: 52, Mean Generator Loss: 51.091468811035156, Mean Discriminator Loss: -1.1859376430511475\nEpoch: 53, Mean Generator Loss: 39.9417724609375, Mean Discriminator Loss: -2.908231735229492\nEpoch: 54, Mean Generator Loss: 33.04358673095703, Mean Discriminator Loss: -0.904320478439331\nEpoch: 55, Mean Generator Loss: 15.860361099243164, Mean Discriminator Loss: -0.5947536826133728\nEpoch: 56, Mean Generator Loss: 14.164628028869629, Mean Discriminator Loss: -4.074370861053467\nEpoch: 57, Mean Generator Loss: 31.019929885864258, Mean Discriminator Loss: -2.6140313148498535\nEpoch: 58, Mean Generator Loss: 53.740272521972656, Mean Discriminator Loss: -2.8719494342803955\nEpoch: 59, Mean Generator Loss: 28.392494201660156, Mean Discriminator Loss: -0.23618285357952118\nEpoch: 60, Mean Generator Loss: 27.463886260986328, Mean Discriminator Loss: -3.3884453773498535\nEpoch: 61, Mean Generator Loss: 34.800167083740234, Mean Discriminator Loss: -1.7610764503479004\nEpoch: 62, Mean Generator Loss: 54.274810791015625, Mean Discriminator Loss: -2.0591447353363037\nEpoch: 63, Mean Generator Loss: 36.58665084838867, Mean Discriminator Loss: -0.8383299112319946\nEpoch: 64, Mean Generator Loss: 41.19013214111328, Mean Discriminator Loss: -3.924121141433716\nEpoch: 65, Mean Generator Loss: 69.41919708251953, Mean Discriminator Loss: 0.3071655035018921\nEpoch: 66, Mean Generator Loss: 25.47874641418457, Mean Discriminator Loss: -4.376833915710449\nEpoch: 67, Mean Generator Loss: 26.278139114379883, Mean Discriminator Loss: -4.830549716949463\nEpoch: 68, Mean Generator Loss: 47.41157531738281, Mean Discriminator Loss: -1.4420785903930664\nEpoch: 69, Mean Generator Loss: 68.320556640625, Mean Discriminator Loss: -2.22261643409729\nEpoch: 70, Mean Generator Loss: 60.1431999206543, Mean Discriminator Loss: -2.500918388366699\nEpoch: 71, Mean Generator Loss: 57.59597396850586, Mean Discriminator Loss: -3.2388336658477783\nEpoch: 72, Mean Generator Loss: 42.35459518432617, Mean Discriminator Loss: -1.65147864818573\nEpoch: 73, Mean Generator Loss: 68.70894622802734, Mean Discriminator Loss: -7.188437461853027\nEpoch: 74, Mean Generator Loss: 84.36943817138672, Mean Discriminator Loss: -1.4557209014892578\nEpoch: 75, Mean Generator Loss: 59.34128189086914, Mean Discriminator Loss: -2.427077293395996\nEpoch: 76, Mean Generator Loss: 52.18831253051758, Mean Discriminator Loss: -2.8211896419525146\nEpoch: 77, Mean Generator Loss: 70.0204086303711, Mean Discriminator Loss: -6.799334526062012\nEpoch: 78, Mean Generator Loss: 70.34394836425781, Mean Discriminator Loss: -7.879637241363525\nEpoch: 79, Mean Generator Loss: 74.50387573242188, Mean Discriminator Loss: -3.049636125564575\nEpoch: 80, Mean Generator Loss: 76.33533477783203, Mean Discriminator Loss: -1.5747394561767578\nEpoch: 81, Mean Generator Loss: 91.9752197265625, Mean Discriminator Loss: -3.853685140609741\nEpoch: 82, Mean Generator Loss: 59.59931945800781, Mean Discriminator Loss: -10.110978126525879\nEpoch: 83, Mean Generator Loss: 70.21961975097656, Mean Discriminator Loss: -10.904553413391113\nEpoch: 84, Mean Generator Loss: 53.642738342285156, Mean Discriminator Loss: -11.434094429016113\nEpoch: 85, Mean Generator Loss: 64.18345642089844, Mean Discriminator Loss: -4.291009426116943\nEpoch: 86, Mean Generator Loss: 53.61310577392578, Mean Discriminator Loss: -5.961315631866455\nEpoch: 87, Mean Generator Loss: 69.76768493652344, Mean Discriminator Loss: -6.884476661682129\nEpoch: 88, Mean Generator Loss: 127.10706329345703, Mean Discriminator Loss: 1.2112854719161987\nEpoch: 89, Mean Generator Loss: 76.18909454345703, Mean Discriminator Loss: -5.2918782234191895\nEpoch: 90, Mean Generator Loss: 93.7216567993164, Mean Discriminator Loss: -5.806509017944336\nEpoch: 91, Mean Generator Loss: 86.43500518798828, Mean Discriminator Loss: -2.0415027141571045\nEpoch: 92, Mean Generator Loss: 86.5057373046875, Mean Discriminator Loss: -1.9741311073303223\nEpoch: 93, Mean Generator Loss: 161.33351135253906, Mean Discriminator Loss: -3.434922933578491\nEpoch: 94, Mean Generator Loss: 94.42237091064453, Mean Discriminator Loss: -8.473536491394043\nEpoch: 95, Mean Generator Loss: 118.08096313476562, Mean Discriminator Loss: -6.746964931488037\nEpoch: 96, Mean Generator Loss: 104.23990631103516, Mean Discriminator Loss: -4.938411712646484\nEpoch: 97, Mean Generator Loss: 92.69732666015625, Mean Discriminator Loss: -9.915912628173828\nEpoch: 98, Mean Generator Loss: 87.30255126953125, Mean Discriminator Loss: -3.808485746383667\nEpoch: 99, Mean Generator Loss: 81.84051513671875, Mean Discriminator Loss: -6.1775946617126465\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store the mean losses for each epoch\n",
    "mean_gen_losses = []\n",
    "mean_disc_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Initialize lists to store losses for each batch\n",
    "    batch_gen_losses = []\n",
    "    batch_disc_losses = []\n",
    "    \n",
    "    for real_data, classes in dataset:\n",
    "        # Perform the training step and get the losses\n",
    "        gen_loss, disc_loss = train_step(real_data, classes, generator, discriminator, generator_optimizer, discriminator_optimizer, batch_size, noise_dim)\n",
    "        \n",
    "        # Append the losses to the batch-specific lists\n",
    "        batch_gen_losses.append(gen_loss.numpy())  # Convert to numpy if it's a tensor\n",
    "        batch_disc_losses.append(disc_loss.numpy())\n",
    "        \n",
    "    # Calculate the mean losses for the epoch\n",
    "    epoch_gen_loss = np.mean(batch_gen_losses)\n",
    "    epoch_disc_loss = np.mean(batch_disc_losses)\n",
    "    \n",
    "    # Log the mean losses for the epoch\n",
    "    mlflow.log_metric(\"Generator Loss\", epoch_gen_loss, step=epoch)\n",
    "    mlflow.log_metric(\"Discriminator Loss\", epoch_disc_loss, step=epoch)\n",
    "    \n",
    "    # Append the mean losses to the epoch-specific lists\n",
    "    mean_gen_losses.append(epoch_gen_loss)\n",
    "    mean_disc_losses.append(epoch_disc_loss)\n",
    "    \n",
    "    # Log the mean losses for monitoring\n",
    "    print(f\"Epoch: {epoch}, Mean Generator Loss: {epoch_gen_loss}, Mean Discriminator Loss: {epoch_disc_loss}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cGAN-based_IDS_tf",
   "widgets": {
    "mlflow_exp_root_path": {
     "currentValue": "/Users/m332479@azg.pwus.us/ml_experments",
     "nuid": "72a23513-d366-4fba-a71d-0d05aec9bffe",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Users/m332479@azg.pwus.us/ml_experments",
      "label": null,
      "name": "mlflow_exp_root_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
